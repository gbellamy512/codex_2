{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bm858-q8bc13"
   },
   "outputs": [],
   "source": [
    "# @title load libraries and mount colab\n",
    "# set seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "import torch as th\n",
    "\n",
    "import random\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "# https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb\n",
    "\n",
    "import pprint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "\n",
    "# !pip install wandb\n",
    "!pip install wandb -Uq\n",
    "# !pip install --upgrade wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from google.colab import userdata\n",
    "wandb.login(key=userdata.get('wandb'))\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_Rnl8aRwAz3"
   },
   "outputs": [],
   "source": [
    "# @title load data\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Load Data (outside of the function; do this once at notebook start)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "first_year = 2013\n",
    "current_year = 2024\n",
    "include_last_week = True\n",
    "\n",
    "# Historical + Current Year EPA data\n",
    "data_hist = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/ewma_epa_df_2006_{current_year-1}.csv'\n",
    ")\n",
    "data_cy = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/ewma_epa_df_{current_year}.csv'\n",
    ")\n",
    "data = pd.concat([data_hist, data_cy])\n",
    "\n",
    "if include_last_week:\n",
    "  data = data.loc[\n",
    "      ((data['season'] < 2022) & (data['week'] <= 17))\n",
    "      | ((data['season'] >= 2022) & (data['week'] <= 18))\n",
    "  ]\n",
    "else:\n",
    "  data = data.loc[\n",
    "    ((data['season'] < 2022) & (data['week'] <= 16))\n",
    "    | ((data['season'] >= 2022) & (data['week'] <= 17))\n",
    "]\n",
    "\n",
    "# Pass Rate data\n",
    "pass_rates_hist = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/pass_rates_{first_year}_{current_year-1}.csv'\n",
    ")\n",
    "pass_rates_cy = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/pass_rates_{current_year}.csv'\n",
    ")\n",
    "pass_rates_hist_ancient = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/pass_rates_2006_2012.csv'\n",
    ")\n",
    "pass_rates = pd.concat([\n",
    "    pass_rates_hist_ancient, pass_rates_hist, pass_rates_cy\n",
    "])\n",
    "\n",
    "# Win percentage data\n",
    "win_percentages_hist = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/weekly_records_1999_{current_year-1}.csv'\n",
    ")\n",
    "win_percentages_cy = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/weekly_records_{current_year}.csv'\n",
    ")\n",
    "win_percentages = pd.concat([win_percentages_hist, win_percentages_cy])\n",
    "\n",
    "# Schedules data\n",
    "schedules_hist = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/nfl_schedules_1999_{current_year-1}.csv'\n",
    ")\n",
    "schedules_cy = pd.read_csv(\n",
    "    f'/content/drive/My Drive/nfl_data/nfl_schedules_{current_year}.csv'\n",
    ")\n",
    "schedules = pd.concat([schedules_hist, schedules_cy])\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. Helper Functions\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def build_full_schedule(df, team_col='team', season_weeks=17):\n",
    "    \"\"\"\n",
    "    Creates a complete schedule of season x [1..season_weeks] x teams.\n",
    "    This helps to fill missing rows for bye weeks, etc.\n",
    "    \"\"\"\n",
    "    teams = df[team_col].unique()\n",
    "    seasons = df['season'].unique()\n",
    "    # Create an index for all (season, week, team) combinations\n",
    "    schedule_idx = pd.MultiIndex.from_product(\n",
    "        [seasons, range(1, season_weeks + 1), teams],\n",
    "        names=['season', 'week', team_col]\n",
    "    )\n",
    "    return schedule_idx.to_frame(index=False)\n",
    "\n",
    "def merge_and_ffill(df, schedule, group_cols, merge_cols, value_col):\n",
    "    \"\"\"\n",
    "    Merges `df` with a complete `schedule` to fill missing weeks/rows,\n",
    "    then forward-fills the specified `value_col`.\n",
    "    After that, increments the 'week' by 1 (so it applies to the next game).\n",
    "    \"\"\"\n",
    "    merged = pd.merge(schedule, df, on=merge_cols, how='left')\n",
    "    # Forward-fill for each (season, team)\n",
    "    merged[value_col] = merged.groupby(group_cols)[value_col].ffill()\n",
    "    # Shift week by 1 so it applies to the next matchup\n",
    "    merged['week'] += 1\n",
    "    return merged\n",
    "\n",
    "def add_h2h(data):\n",
    "    \"\"\"\n",
    "    For each row, look up the most recent head-to-head matchup\n",
    "    and determine which team won. Adds 'h2h_type' to the dataframe:\n",
    "      - 'home' if last time the home team won\n",
    "      - 'away' if last time the away team won\n",
    "      - 'tie' if last time was a tie\n",
    "      - 'na' if there was no last time (no previous matchup)\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    data['h2h_team'] = None\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        season = row['season']\n",
    "        home_team = row['home_team']\n",
    "        away_team = row['away_team']\n",
    "        week = row['week']\n",
    "\n",
    "        # Filter: same season, same matchup, prior week\n",
    "        relevant_games = data[\n",
    "            (data['season'] == season)\n",
    "            & (\n",
    "                ((data['home_team'] == home_team) & (data['away_team'] == away_team)) |\n",
    "                ((data['home_team'] == away_team) & (data['away_team'] == home_team))\n",
    "            )\n",
    "            & (data['week'] < week)\n",
    "        ]\n",
    "\n",
    "        if not relevant_games.empty:\n",
    "            last_game = relevant_games.sort_values(by='week', ascending=False).iloc[0]\n",
    "\n",
    "            if last_game['home_score'] > last_game['away_score']:\n",
    "                data.at[index, 'h2h_team'] = last_game['home_team']\n",
    "            elif last_game['away_score'] > last_game['home_score']:\n",
    "                data.at[index, 'h2h_team'] = last_game['away_team']\n",
    "            else:\n",
    "                data.at[index, 'h2h_team'] = \"tie\"\n",
    "\n",
    "    # Map the winner to 'home', 'away', or 'tie'\n",
    "    def assign_h2h_type(row):\n",
    "        if pd.isna(row['h2h_team']):\n",
    "            return 'na'\n",
    "        elif row['h2h_team'] == 'tie':\n",
    "            return 'tie'\n",
    "        elif row['h2h_team'] == row['home_team']:\n",
    "            return 'home'\n",
    "        else:\n",
    "            return 'away'\n",
    "\n",
    "    data['h2h_type'] = data.apply(assign_h2h_type, axis=1)\n",
    "    data.drop(columns=['h2h_team'], inplace=True)\n",
    "    return data\n",
    "\n",
    "def determine_favorite(row):\n",
    "    \"\"\"\n",
    "    Determine which team is favored based on moneyline.\n",
    "      - If home_moneyline < away_moneyline -> home is favorite, away is dog\n",
    "      - If away_moneyline < home_moneyline -> away is favorite, home is dog\n",
    "      - If equal, break ties by sign convention (negative means favorite).\n",
    "    \"\"\"\n",
    "    if row['home_moneyline'] < row['away_moneyline']:\n",
    "        return 'home', 'away'\n",
    "    elif row['away_moneyline'] < row['home_moneyline']:\n",
    "        return 'away', 'home'\n",
    "    else:\n",
    "        # Tie moneylines: fallback\n",
    "        return ('home', 'away') if row['home_moneyline'] < 0 else ('away', 'home')\n",
    "\n",
    "def implied_probability(moneyline):\n",
    "    \"\"\"\n",
    "    Converts American moneyline odds to implied probability.\n",
    "    \"\"\"\n",
    "    if moneyline < 0:\n",
    "        return -moneyline / (-moneyline + 100)\n",
    "    else:\n",
    "        return 100 / (moneyline + 100)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. Main Function: prepare_df\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def prepare_df(\n",
    "    min_periods=3,\n",
    "    span=8,\n",
    "    avg_method='ewma',  # new parameter to choose averaging method\n",
    "    data=data.copy(),\n",
    "    pass_rates=pass_rates.copy(),\n",
    "    win_percentages=win_percentages.copy(),\n",
    "    schedules=schedules.copy()\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare a single dataframe that merges:\n",
    "      - Weighted EPA data (already loaded + filtered globally)\n",
    "      - Pass rates (forward-filled to handle bye weeks)\n",
    "      - Win percentages (forward-filled)\n",
    "      - Additional scheduling, moneyline, rest data\n",
    "      - Creates columns for favorite/dog, h2h outcome, rest advantage, etc.\n",
    "\n",
    "    The avg_method parameter can be either 'ewma' or 'simple'.\n",
    "    'ewma' (default) uses exponentially weighted moving averages,\n",
    "    while 'simple' uses the season-level simple averages (e.g., columns like rushing_offense_epa_season).\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    # -------------- COLUMN SELECTION --------------\n",
    "    # Choose the metric substring based on the averaging method.\n",
    "    if avg_method == 'ewma':\n",
    "        metric = f'ewma_{min_periods}min_{span}span'\n",
    "    elif avg_method == 'simple':\n",
    "        metric = 'epa_season'\n",
    "    else:\n",
    "        raise ValueError(\"avg_method must be either 'ewma' or 'simple'\")\n",
    "\n",
    "    # Keep only columns for modeling and 'home_team_win'\n",
    "    keep_cols = [\n",
    "        c for c in df.columns\n",
    "        if c == 'home_team_win'\n",
    "        or metric in c\n",
    "        or c in ['season', 'week', 'home_team', 'away_team']\n",
    "    ]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # -------------- PASS RATES --------------\n",
    "    pass_rates_sched = build_full_schedule(pass_rates, 'team', season_weeks=17)\n",
    "    pass_rates_merged = merge_and_ffill(\n",
    "        pass_rates,\n",
    "        pass_rates_sched,\n",
    "        group_cols=['season', 'team'],\n",
    "        merge_cols=['season', 'week', 'team'],\n",
    "        value_col='pass_rate_ytd'\n",
    "    )\n",
    "    # Merge pass_rate_ytd for home and away teams\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        pass_rates_merged.rename(columns={\n",
    "            'team': 'home_team',\n",
    "            'pass_rate_ytd': 'pass_rate_ytd_home'\n",
    "        }),\n",
    "        on=['season', 'week', 'home_team'],\n",
    "        how='left'\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        pass_rates_merged.rename(columns={\n",
    "            'team': 'away_team',\n",
    "            'pass_rate_ytd': 'pass_rate_ytd_away'\n",
    "        }),\n",
    "        on=['season', 'week', 'away_team'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------- WIN PERCENTAGES --------------\n",
    "    win_perc_sched = build_full_schedule(win_percentages, 'team', season_weeks=17)\n",
    "    win_perc_merged = merge_and_ffill(\n",
    "        win_percentages,\n",
    "        win_perc_sched,\n",
    "        group_cols=['season', 'team'],\n",
    "        merge_cols=['season', 'week', 'team'],\n",
    "        value_col='win_percentage'\n",
    "    )\n",
    "    win_perc_merged = win_perc_merged[['season', 'week', 'team', 'win_percentage']]\n",
    "\n",
    "    # Merge win_percentage for home and away teams\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        win_perc_merged.rename(columns={\n",
    "            'team': 'home_team',\n",
    "            'win_percentage': 'win_percentage_home'\n",
    "        }),\n",
    "        on=['season', 'week', 'home_team'],\n",
    "        how='left'\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        win_perc_merged.rename(columns={\n",
    "            'team': 'away_team',\n",
    "            'win_percentage': 'win_percentage_away'\n",
    "        }),\n",
    "        on=['season', 'week', 'away_team'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------- SCHEDULES / MONEYLINES / REST --------------\n",
    "    schedules = schedules[['season', 'week', 'home_team', 'away_team',\n",
    "                            'home_score', 'away_score',\n",
    "                            'home_moneyline', 'away_moneyline',\n",
    "                            'home_rest', 'away_rest', 'div_game', 'weekday']].copy()\n",
    "    schedules['weekday'] = schedules['weekday'].apply(\n",
    "        lambda x: x if x in ['Monday', 'Thursday', 'Sunday'] else 'other'\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        schedules,\n",
    "        on=['season', 'week', 'home_team', 'away_team'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # -------------- HEAD-TO-HEAD COLUMN --------------\n",
    "    df = add_h2h(df)\n",
    "\n",
    "    # Drop rows that have missing data after merges\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # -------------- FAVORITE / DOG IDENTIFICATION --------------\n",
    "    df[['favorite', 'dog']] = df.apply(determine_favorite, axis=1, result_type=\"expand\")\n",
    "    df['dog_home'] = df['favorite'].apply(lambda x: 1 if x == 'away' else 0)\n",
    "\n",
    "    def map_h2h_type(row):\n",
    "        if row['h2h_type'] == 'home':\n",
    "            return 'dog' if row['dog_home'] else 'fav'\n",
    "        elif row['h2h_type'] == 'away':\n",
    "            return 'fav' if row['dog_home'] else 'dog'\n",
    "        else:\n",
    "            return 'na'\n",
    "\n",
    "    df['h2h_type'] = df.apply(map_h2h_type, axis=1)\n",
    "\n",
    "    # -------------- FAVORITE / DOG COLUMNS --------------\n",
    "    home_cols = [col for col in df.columns if 'home' in col]\n",
    "    away_cols = [col for col in df.columns if 'away' in col]\n",
    "\n",
    "    for home_col in home_cols:\n",
    "        away_col = home_col.replace('home', 'away')\n",
    "        fav_col  = home_col.replace('home', 'fav')\n",
    "        dog_col  = home_col.replace('home', 'dog')\n",
    "\n",
    "        if away_col in df.columns:\n",
    "            df[fav_col] = df.apply(\n",
    "                lambda row: row[home_col] if row['favorite'] == 'home' else row[away_col],\n",
    "                axis=1\n",
    "            )\n",
    "            df[dog_col] = df.apply(\n",
    "                lambda row: row[away_col] if row['favorite'] == 'home' else row[home_col],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "    df.drop(columns=(home_cols + away_cols + ['favorite', 'dog']), inplace=True)\n",
    "\n",
    "    # -------------- TARGET: DID THE DOG WIN? --------------\n",
    "    df['dog_win'] = df.apply(lambda row: 1 if row['dog_score'] > row['fav_score'] else 0, axis=1)\n",
    "\n",
    "    # -------------- IMPLIED PROBABILITIES --------------\n",
    "    df['fav_implied_prob'] = df['fav_moneyline'].apply(implied_probability)\n",
    "    df['dog_implied_prob'] = df['dog_moneyline'].apply(implied_probability)\n",
    "\n",
    "    # -------------- WEEKDAY INDICATOR --------------\n",
    "    df['sunday'] = df['weekday'].apply(lambda x: 1 if x == 'Sunday' else 0)\n",
    "\n",
    "    # -------------- ADVANTAGE COLUMNS --------------\n",
    "    if avg_method == 'ewma':\n",
    "        # Use the existing exponentially weighted moving average columns\n",
    "        df['rushing_offense_adv'] = (\n",
    "            df[f'ewma_{min_periods}min_{span}span_rushing_offense_net_dog']\n",
    "            - df[f'ewma_{min_periods}min_{span}span_rushing_defense_net_fav']\n",
    "        )\n",
    "        df['passing_offense_adv'] = (\n",
    "            df[f'ewma_{min_periods}min_{span}span_passing_offense_net_dog']\n",
    "            - df[f'ewma_{min_periods}min_{span}span_passing_defense_net_fav']\n",
    "        )\n",
    "        df['rushing_defense_adv'] = (\n",
    "            df[f'ewma_{min_periods}min_{span}span_rushing_defense_net_dog']\n",
    "            - df[f'ewma_{min_periods}min_{span}span_rushing_offense_net_fav']\n",
    "        )\n",
    "        df['passing_defense_adv'] = (\n",
    "            df[f'ewma_{min_periods}min_{span}span_passing_defense_net_dog']\n",
    "            - df[f'ewma_{min_periods}min_{span}span_passing_offense_net_fav']\n",
    "        )\n",
    "    else:\n",
    "        df['rushing_offense_adv'] = (\n",
    "            df['rushing_offense_epa_season_dog']\n",
    "            + df['rushing_defense_epa_season_fav']\n",
    "        )\n",
    "        df['passing_offense_adv'] = (\n",
    "            df['passing_offense_epa_season_dog']\n",
    "            + df['passing_defense_epa_season_fav']\n",
    "        )\n",
    "        df['rushing_defense_adv'] = (\n",
    "            -1 * df['rushing_defense_epa_season_dog']\n",
    "            - df['rushing_offense_epa_season_fav']\n",
    "        )\n",
    "        df['passing_defense_adv'] = (\n",
    "            -1 * df['passing_defense_epa_season_dog']\n",
    "            - df['passing_offense_epa_season_fav']\n",
    "        )\n",
    "\n",
    "    # -------------- OTHER DERIVED FEATURES --------------\n",
    "    df['win_percentage_diff'] = df['win_percentage_dog'] - df['win_percentage_fav']\n",
    "    df['implied_prob_diff']   = df['dog_implied_prob'] - df['fav_implied_prob']\n",
    "    df['rest_advantage']      = df['dog_rest'] - df['fav_rest']\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. Run the function\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# avg_method = 'ewma'\n",
    "avg_method = 'simple'\n",
    "df = prepare_df(avg_method=avg_method)\n",
    "features = [\n",
    "    f'rushing_offense_adv',\n",
    "    f'passing_offense_adv',\n",
    "    f'rushing_defense_adv',\n",
    "    f'passing_defense_adv',\n",
    "    'win_percentage_diff',\n",
    "    'implied_prob_diff',\n",
    "    'rest_advantage',\n",
    "    'div_game',\n",
    "    'h2h_type',\n",
    "]\n",
    "\n",
    "\n",
    "categorical_features  = ['h2h_type']\n",
    "binary_features = ['div_game']\n",
    "non_numerical_features = binary_features + categorical_features\n",
    "numerical_features = [feature for feature in features if feature not in non_numerical_features]\n",
    "\n",
    "cy = 2024\n",
    "cy_df = df[df['season'] == cy]\n",
    "df = df[df['season'] < cy]\n",
    "\n",
    "print('before current year')\n",
    "print(len(df))\n",
    "print(df['dog_win'].value_counts(normalize=True))\n",
    "print('')\n",
    "print('current year')\n",
    "print(len(cy_df))\n",
    "print(cy_df['dog_win'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDkshifV_WcS"
   },
   "outputs": [],
   "source": [
    "from re import VERBOSE\n",
    "# @title create betting logic\n",
    "def implied_probability(moneyline):\n",
    "    \"\"\"\n",
    "    Convert American moneyline odds to implied probability.\n",
    "    \"\"\"\n",
    "    if moneyline > 0:\n",
    "        return 100 / (moneyline + 100)\n",
    "    else:\n",
    "        return -moneyline / (-moneyline + 100)\n",
    "\n",
    "def calculate_implied_probabilities(df, fav_col='fav_moneyline', dog_col='dog_moneyline'):\n",
    "    \"\"\"\n",
    "    Append columns with implied probabilities for the favorite and underdog.\n",
    "    \"\"\"\n",
    "    df['fav_implied_prob'] = df[fav_col].apply(implied_probability)\n",
    "    df['dog_implied_prob'] = df[dog_col].apply(implied_probability)\n",
    "    return df\n",
    "\n",
    "def determine_bet_team(row, margin=0.0):\n",
    "    \"\"\"\n",
    "    Logic to decide which team to bet on based on:\n",
    "      - model prediction\n",
    "      - implied probabilities\n",
    "      - margin\n",
    "    Returns 'dog', 'fav', or 'none'.\n",
    "    \"\"\"\n",
    "    # If the dog\u2019s predicted win probability exceeds dog_implied_prob + margin\n",
    "    # we bet on the dog.\n",
    "    if row['predictions'] > (row['dog_implied_prob'] + margin):\n",
    "        return 'dog'\n",
    "    # If the favorite\u2019s predicted win probability exceeds fav_implied_prob + margin\n",
    "    # i.e. (1 - row['predictions']) > (row['fav_implied_prob'] + margin)\n",
    "    elif (1 - row['predictions']) > (row['fav_implied_prob'] + margin):\n",
    "        return 'fav'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "def simple_bet_team(row):\n",
    "    \"\"\"\n",
    "    If the dog's predicted win probability is > 0.5, bet on the dog;\n",
    "    otherwise, bet on the favorite.\n",
    "    \"\"\"\n",
    "    return 'dog' if row['predictions'] > 0.5 else 'fav'\n",
    "\n",
    "def calculate_stake(row, bet_amount, bet_strat='both'):\n",
    "    \"\"\"\n",
    "    Determine the stake amount (0 or bet_amount) depending on bet_strat:\n",
    "      - 'both': bet on whichever team bet_team says (if bet_team != 'none')\n",
    "      - 'dog': only bet if bet_team == 'dog'\n",
    "      - 'fav': only bet if bet_team == 'fav'\n",
    "    \"\"\"\n",
    "    should_bet = False\n",
    "\n",
    "    if bet_strat == 'both':\n",
    "        should_bet = (row['bet_team'] != 'none')\n",
    "    elif bet_strat == 'dog':\n",
    "        should_bet = (row['bet_team'] == 'dog')\n",
    "    elif bet_strat == 'fav':\n",
    "        should_bet = (row['bet_team'] == 'fav')\n",
    "\n",
    "    return bet_amount if should_bet else 0\n",
    "\n",
    "def calculate_profit(row):\n",
    "    \"\"\"\n",
    "    Calculate profit (or loss) for a single row, given:\n",
    "      - which team was bet on\n",
    "      - who actually won (dog_win)\n",
    "      - moneyline for each team\n",
    "      - bet amount\n",
    "    \"\"\"\n",
    "    # If we bet on the favorite\n",
    "    if row['bet_team'] == 'fav':\n",
    "        if row['dog_win'] == 0:  # Favorite won\n",
    "            # For negative moneylines, payout is bet * (100 / -moneyline)\n",
    "            # For positive moneylines, payout is bet * (moneyline / 100)\n",
    "            return (\n",
    "                row['bet'] * (100 / -row['fav_moneyline'])\n",
    "                if row['fav_moneyline'] < 0\n",
    "                else row['bet'] * (row['fav_moneyline'] / 100)\n",
    "            )\n",
    "        else:\n",
    "            return -row['bet']\n",
    "\n",
    "    # If we bet on the dog\n",
    "    elif row['bet_team'] == 'dog':\n",
    "        if row['dog_win'] == 1:  # Dog won\n",
    "            return (\n",
    "                row['bet'] * (100 / -row['dog_moneyline'])\n",
    "                if row['dog_moneyline'] < 0\n",
    "                else row['bet'] * (row['dog_moneyline'] / 100)\n",
    "            )\n",
    "        else:\n",
    "            return -row['bet']\n",
    "\n",
    "    # No bet placed\n",
    "    return 0\n",
    "\n",
    "def calculate_dog_fav_profits(df, fav_moneyline_col='fav_moneyline',\n",
    "                              dog_moneyline_col='dog_moneyline',\n",
    "                              dog_win_col='dog_win', bet_amount=100):\n",
    "    \"\"\"\n",
    "    Calculate hypothetical profits if a user always bets on dog or\n",
    "    always bets on fav for each game. (For reference/comparison.)\n",
    "    \"\"\"\n",
    "    # Profit if you always bet on the dog\n",
    "    df['dog_profit'] = df.apply(\n",
    "        lambda row: bet_amount * (row[dog_moneyline_col] / 100)\n",
    "        if row[dog_win_col] == 1 else -bet_amount,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Profit if you always bet on the favorite\n",
    "    df['fav_profit'] = df.apply(\n",
    "        lambda row: bet_amount * (100 / abs(row[fav_moneyline_col]))\n",
    "        if row[dog_win_col] == 0 else -bet_amount,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_betting_strategy(df,\n",
    "                              model,\n",
    "                              features,\n",
    "                              pipeline,\n",
    "                              bet_amount=100,\n",
    "                              target='dog_win',\n",
    "                              use_implied_prob=True,\n",
    "                              bet_strat='both',\n",
    "                              margin=0.0,\n",
    "                              # suppresses the bars\n",
    "                              keras_verbose=0):\n",
    "\n",
    "    # 1. Prepare input features\n",
    "    X = df[features]\n",
    "\n",
    "    X_norm = pipeline.transform(X)\n",
    "\n",
    "    # 2. Predict dog\u2019s win probability\n",
    "    # predictions = model.predict(X_norm)\n",
    "    predictions = model.predict(X_norm, verbose=keras_verbose)\n",
    "    df_eval = df.copy()  # Work off a copy to avoid modifying original DataFrame\n",
    "    df_eval['predictions'] = predictions\n",
    "\n",
    "    # 3. Calculate implied probabilities\n",
    "    df_eval = calculate_implied_probabilities(df_eval)\n",
    "\n",
    "    # 4. Determine which team to bet on\n",
    "    if use_implied_prob:\n",
    "        df_eval['bet_team'] = df_eval.apply(\n",
    "            determine_bet_team, margin=margin, axis=1\n",
    "        )\n",
    "    else:\n",
    "        df_eval['bet_team'] = df_eval.apply(simple_bet_team, axis=1)\n",
    "\n",
    "    # 5. Decide bet amount based on bet_strat\n",
    "    df_eval['bet'] = df_eval.apply(\n",
    "        lambda row: calculate_stake(row, bet_amount, bet_strat),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 6. Calculate profit or loss for each game\n",
    "    df_eval['profit'] = df_eval.apply(calculate_profit, axis=1)\n",
    "\n",
    "    # 7. Summaries of overall profit / ROI\n",
    "    total_profit = df_eval['profit'].sum()\n",
    "    total_bet = df_eval['bet'].sum()\n",
    "    return_on_investment = (total_profit / total_bet * 100) if total_bet else 0.0\n",
    "\n",
    "    # 8. Calculate hypothetical \u201calways dog\u201d or \u201calways fav\u201d returns\n",
    "    df_eval = calculate_dog_fav_profits(df_eval,\n",
    "                                        fav_moneyline_col='fav_moneyline',\n",
    "                                        dog_moneyline_col='dog_moneyline',\n",
    "                                        dog_win_col=target,\n",
    "                                        bet_amount=bet_amount)\n",
    "\n",
    "    total_bet_everything = len(df_eval) * bet_amount\n",
    "    fav_profit = df_eval['fav_profit'].sum()\n",
    "    fav_return_on_investment = (fav_profit / total_bet_everything * 100)\n",
    "\n",
    "    dog_profit = df_eval['dog_profit'].sum()\n",
    "    dog_return_on_investment = (dog_profit / total_bet_everything * 100)\n",
    "\n",
    "    # 9. Dog win rate\n",
    "    dog_win_rate = (df_eval[df_eval[target] == 1].shape[0] / len(df_eval)\n",
    "                    if len(df_eval) > 0 else 0.0)\n",
    "\n",
    "    # 10. Compile results into a dictionary\n",
    "    results = {\n",
    "        \"total_profit\": total_profit,\n",
    "        \"total_bet\": total_bet,\n",
    "        \"roi\": return_on_investment,\n",
    "        # \"fav_roi\": fav_return_on_investment,\n",
    "        # \"dog_roi\": dog_return_on_investment,\n",
    "        \"dog_win_rate\": dog_win_rate,\n",
    "        # Return the full evaluation DataFrame in case you need details\n",
    "        \"df\": df_eval,\n",
    "        'bet_rate': 100 * len(df_eval[df_eval['bet'] > 0]) / len(df_eval)\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1XAcYWXHhBy"
   },
   "source": [
    "gemini\n",
    "\n",
    "I would highly recommend starting with:\n",
    "\n",
    "Optimizer: adam\n",
    "Activation: relu\n",
    "Initializer: he_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLIQ4m28Orki"
   },
   "outputs": [],
   "source": [
    "# @title run sweep\n",
    "\n",
    "wandb_project = 'nfl_bet_sweep_2'\n",
    "project_name = wandb_project\n",
    "\n",
    "def create_sweep():\n",
    "\n",
    "  sweep_config = {\n",
    "      # 'method': 'random'\n",
    "      'method': 'bayes'\n",
    "  }\n",
    "\n",
    "  metric = {\n",
    "      # 'name': 'test_loss',\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'\n",
    "  }\n",
    "\n",
    "  sweep_config['metric'] = metric\n",
    "\n",
    "  parameters_dict = {\n",
    "      # 'optimizer': {\n",
    "      #     'values': ['adam', 'sgd', 'rmsprop']\n",
    "      # },\n",
    "      # 'sgd_momentum': {\n",
    "      #     'values': [0.0, 0.5, 0.9]\n",
    "      # },\n",
    "      # 'kernel_initializer': {\n",
    "      #     'values': [\n",
    "      #         'he_normal',\n",
    "      #         'glorot_normal',\n",
    "      #     ]\n",
    "      # },\n",
    "      # 'activation': {\n",
    "      #     'values': [\n",
    "      #         'relu',\n",
    "      #         'elu',\n",
    "      #     ]\n",
    "      # },\n",
    "      # 'learning_rate': {\n",
    "      #     'values': [0.01, 0.001, 0.0005, 0.0001]\n",
    "      # },\n",
    "      'learning_rate': {\n",
    "          'distribution': 'log_uniform_values',\n",
    "          'min': 1e-5,\n",
    "          'max': 1e-2\n",
    "      },\n",
    "      # 'epochs': {\n",
    "      #     # 'values': [25, 50, 75, 100]\n",
    "      #     'values': [200]\n",
    "      # },\n",
    "      'batch_size': {\n",
    "          'values': [16, 32, 64]\n",
    "      },\n",
    "      # 'hidden_layers': {\n",
    "      #     'values': [2, 3, 4, 5]\n",
    "      # },\n",
    "      'hidden_layers': {\n",
    "          'distribution': 'int_uniform',\n",
    "          'min': 2,\n",
    "          'max': 4\n",
    "      },\n",
    "      'neurons': {\n",
    "          'values': [16, 32, 64]\n",
    "      },\n",
    "      'batch_norm': {\n",
    "          'values': [True, False]\n",
    "      },\n",
    "      # 'l1_reg': {\n",
    "      #     'values': [0.0, 0.001, 0.01, 0.1]\n",
    "      # },\n",
    "      # 'l2_reg': {\n",
    "      #     'values': [0.0, 0.001, 0.01, 0.1]\n",
    "      # },\n",
    "      # 'dropout': {\n",
    "      #     'values': [0.0, 0.1, 0.2, 0.3]\n",
    "      # },\n",
    "      'l1_reg': {\n",
    "          'distribution': 'log_uniform_values',\n",
    "          'min': 1e-6,\n",
    "          'max': 1e-2\n",
    "      },\n",
    "      'l2_reg': {\n",
    "          'distribution': 'log_uniform_values',\n",
    "          'min': 1e-6,\n",
    "          'max': 1e-2\n",
    "      },\n",
    "      'dropout': {\n",
    "          'distribution': 'uniform',\n",
    "          'min': 0.0,\n",
    "          'max': 0.5\n",
    "      },\n",
    "      'apply_class_weights': {\n",
    "          'values': [True, False]\n",
    "      },\n",
    "      # 'min_periods': {\n",
    "      #     'values': [3, 6]\n",
    "      # },\n",
    "      # 'span': {\n",
    "      #     'values': [3, 8, 16]\n",
    "      # },\n",
    "      # 'early_stopping': {\n",
    "      #     # 'values': [True, False]\n",
    "      #     'values': [True]\n",
    "      # },\n",
    "      # 'early_stopping_patience': {\n",
    "      #     'values': [10, 20, 30]\n",
    "      # },\n",
    "      'early_stopping_patience': {\n",
    "          'distribution': 'int_uniform',\n",
    "          'min': 10,  # Or a bit lower if you want to explore shorter patience\n",
    "          'max': 35   # Or a bit higher if you want to explore longer patience\n",
    "      },\n",
    "      # 'early_stopping_min_delta': {\n",
    "      #     'values': [0.001, 0.0001, 0.00001]\n",
    "      # },\n",
    "      'early_stopping_min_delta': {\n",
    "          'distribution': 'log_uniform_values',\n",
    "          'min': 1e-5,  # log(0.00001)\n",
    "          'max': 1e-3   # log(0.001)\n",
    "      },\n",
    "      # 'apply_lr_schedule': {\n",
    "      #     # 'values': [True, False]\n",
    "      #     'values': [True]\n",
    "      # },\n",
    "      # 'lr_scheduler_step_every': {\n",
    "      #     'values': [5, 10, 25]\n",
    "      # },\n",
    "      'lr_scheduler_step_every': {\n",
    "          'distribution': 'int_uniform',\n",
    "          'min': 5,\n",
    "          'max': 30 # Or adjust based on your typical total epochs\n",
    "      },\n",
    "      # 'lr_scheduler_step_factor': {\n",
    "      #     'values': [0.25, 0.5, 0.75]\n",
    "      # }\n",
    "      'lr_scheduler_step_factor': {\n",
    "          'distribution': 'uniform', # or 'log_uniform'\n",
    "          'min': 0.1,  # More aggressive decay\n",
    "          'max': 0.8   # More gentle decay\n",
    "      }\n",
    "  }\n",
    "\n",
    "  sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "  gpu_name = th.cuda.get_device_name(th.cuda.current_device()) if th.cuda.is_available() else 'CPU'\n",
    "\n",
    "  # Add fixed-value parameters\n",
    "  parameters_dict.update({\n",
    "      'gpu_name': {\n",
    "          'value': gpu_name\n",
    "      },\n",
    "      'target': {\n",
    "          'value': 'dog_win'\n",
    "      },\n",
    "      'loss': {\n",
    "          'value': 'binary_crossentropy'\n",
    "      },\n",
    "      'metric': {\n",
    "          'value': 'accuracy'\n",
    "      },\n",
    "     'optimizer': {\n",
    "          'value': 'adam'\n",
    "      },\n",
    "      'kernel_initializer': {\n",
    "          'value': 'he_normal'\n",
    "      },\n",
    "      'activation': {\n",
    "          'value': 'relu'\n",
    "      },\n",
    "      'outer_activation': {\n",
    "          'value': 'sigmoid'\n",
    "      },\n",
    "      'epochs': {\n",
    "          'value': 200\n",
    "      },\n",
    "      'early_stopping': {\n",
    "          'value': True\n",
    "      },\n",
    "      'apply_lr_schedule': {\n",
    "          'value': True\n",
    "      },\n",
    "      'test_size': {\n",
    "          'value': 0.1\n",
    "      },\n",
    "      'min_periods': {\n",
    "          'value': 3\n",
    "      },\n",
    "      'span': {\n",
    "          'value': 8\n",
    "      },\n",
    "      # 'bet_strat': {\n",
    "      #     'value': 'both'\n",
    "      # },\n",
    "      # 'margin': {\n",
    "      #     'value': 0.00\n",
    "      # },\n",
    "  })\n",
    "\n",
    "  pprint.pprint(sweep_config)\n",
    "\n",
    "  sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "  return sweep_id\n",
    "\n",
    "precision_metric = tf.keras.metrics.Precision()\n",
    "recall_metric = tf.keras.metrics.Recall()\n",
    "\n",
    "def evaluate_and_log_metrics(model, X, y, tag):\n",
    "    # Evaluate the model on the provided data\n",
    "    loss, accuracy, precision, recall = model.evaluate(X, y, verbose=0)\n",
    "\n",
    "    # Calculate F1 Score based on precision and recall\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    # Log metrics with the given tag (train, val, or test)\n",
    "    wandb.log({\n",
    "        f\"{tag}_loss\": loss,\n",
    "        f\"{tag}_accuracy\": accuracy,\n",
    "        f\"{tag}_precision\": precision,\n",
    "        f\"{tag}_recall\": recall,\n",
    "        f\"{tag}_f1\": f1_score,\n",
    "        f\"{tag}_len\": len(X)\n",
    "    })\n",
    "\n",
    "def evaluate_and_log_metrics_betting(dataset, prefix, model, pipeline, features, bet_strat, margin):\n",
    "    results = evaluate_betting_strategy(dataset, model, pipeline=pipeline, features=features,\n",
    "                                        bet_strat=bet_strat, margin=margin)\n",
    "    wandb.log({\n",
    "        f\"{prefix}_profit\": results[\"total_profit\"],\n",
    "        f\"{prefix}_bet\": results[\"total_bet\"],\n",
    "        f\"{prefix}_roi\": results[\"roi\"],\n",
    "    })\n",
    "\n",
    "def get_activation(activation_name):\n",
    "    \"\"\"Helper function to handle activation functions\"\"\"\n",
    "    if activation_name == 'elu':\n",
    "        return tf.keras.activations.elu\n",
    "    return activation_name  # 'relu' and others work directly\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        keras_verbose = 0\n",
    "        save_freq_epochs = 10\n",
    "\n",
    "        # Prepare your dataframe for training\n",
    "        df = prepare_df(min_periods=config.min_periods, span=config.span)\n",
    "\n",
    "        cy = 2024\n",
    "        cy_df = df[df['season'] == cy]\n",
    "        df = df[df['season'] < cy]\n",
    "\n",
    "        # ColumnTransformer to handle numerical and categorical preprocessing\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numerical_features + binary_features),\n",
    "                ('cat', OneHotEncoder(), categorical_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Generate or define a random seed\n",
    "        random_state = random.randint(0, 1_000)\n",
    "        wandb.log({\"random_state\": random_state})\n",
    "\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "            df[features],\n",
    "            df[config.target],\n",
    "            test_size=config.test_size,\n",
    "            stratify=df[config.target],\n",
    "            random_state=random_state\n",
    "        )\n",
    "        X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(\n",
    "            X_train_df,\n",
    "            y_train_df,\n",
    "            test_size=config.test_size,\n",
    "            stratify=y_train_df,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Apply the transformations to the datasets\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "        X_train = pipeline.fit_transform(X_train_df)\n",
    "        X_val = pipeline.transform(X_val_df)\n",
    "        X_test = pipeline.transform(X_test_df)\n",
    "\n",
    "        pipeline_file = \"pipeline.pkl\"\n",
    "        joblib.dump(pipeline, pipeline_file)\n",
    "        artifact = wandb.Artifact(f'preprocessing_pipeline_{wandb.run.id}', type='pipeline')\n",
    "        artifact.add_file(pipeline_file)\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "        df_train = df.loc[X_train_df.index].copy()\n",
    "        df_val = df.loc[X_val_df.index].copy()\n",
    "        df_test = df.loc[X_test_df.index].copy()\n",
    "\n",
    "        y_train = y_train_df.values\n",
    "        y_val = y_val_df.values\n",
    "        y_test = y_test_df.values\n",
    "\n",
    "        # Build model\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Input(shape=(X_train.shape[1],)))\n",
    "        for i in range(config.hidden_layers):\n",
    "            if config.batch_norm:\n",
    "                model.add(tf.keras.layers.BatchNormalization())\n",
    "            model.add(tf.keras.layers.Dropout(config.dropout))\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                config.neurons,\n",
    "                activation=get_activation(config.activation),\n",
    "                kernel_initializer=config.kernel_initializer,\n",
    "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=config.l1_reg, l2=config.l2_reg)\n",
    "            ))\n",
    "        model.add(tf.keras.layers.Dropout(config.dropout))\n",
    "        model.add(tf.keras.layers.Dense(1, activation=config.outer_activation))\n",
    "\n",
    "        # Choose optimizer\n",
    "        if config.optimizer == 'sgd':\n",
    "            momentum_value = float(config.sgd_momentum)\n",
    "            optimizer = tf.keras.optimizers.SGD(\n",
    "                learning_rate=config.learning_rate,\n",
    "                momentum=momentum_value\n",
    "            )\n",
    "        elif config.optimizer == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "        elif config.optimizer == 'rmsprop':\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=config.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {config.optimizer}\")\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=config.loss,\n",
    "            metrics=[config.metric, precision_metric, recall_metric]\n",
    "        )\n",
    "\n",
    "        wandb_callbacks = [WandbMetricsLogger()]\n",
    "\n",
    "        # 2) Early Stopping (if enabled)\n",
    "        if config.early_stopping:\n",
    "            early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=config.early_stopping_patience,\n",
    "                min_delta=config.early_stopping_min_delta,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            wandb_callbacks.append(early_stopping_cb)\n",
    "\n",
    "        # 3) Learning Rate Scheduler (if enabled)\n",
    "        if config.apply_lr_schedule:\n",
    "            # # Example: reduce LR by factor of 10 after 10 epochs\n",
    "            # def scheduler(epoch, lr):\n",
    "            #     # After 10 epochs, reduce LR by a factor of 10\n",
    "            #     return lr * 0.1 if epoch >= 10 else lr\n",
    "            def scheduler(epoch, lr):\n",
    "                \"\"\"Reduce the LR by `factor` every `step_every` epochs.\"\"\"\n",
    "                if (epoch + 1) % config.lr_scheduler_step_every == 0:\n",
    "                    return lr * config.lr_scheduler_step_factor\n",
    "                return lr\n",
    "\n",
    "            lr_scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "            wandb_callbacks.append(lr_scheduler_cb)\n",
    "\n",
    "        # Compute class weights if flagged\n",
    "        if config.apply_class_weights:\n",
    "            class_weights = compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(y_train),\n",
    "                y=y_train\n",
    "            )\n",
    "            class_weight_dict = dict(enumerate(class_weights))\n",
    "        else:\n",
    "            class_weight_dict = None\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            epochs=config.epochs,\n",
    "            batch_size=config.batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=wandb_callbacks,\n",
    "            verbose=keras_verbose,\n",
    "            class_weight=class_weight_dict\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        model_file = \"model.keras\"\n",
    "        model.save(model_file)\n",
    "        artifact = wandb.Artifact(f'model_{wandb.run.id}', type='model')\n",
    "        artifact.add_file(model_file)\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "        # Evaluate and log metrics on train/val/test\n",
    "        evaluate_and_log_metrics(model, X_train, y_train, \"train\")\n",
    "        evaluate_and_log_metrics(model, X_val, y_val, \"val\")\n",
    "        evaluate_and_log_metrics(model, X_test, y_test, \"test\")\n",
    "\n",
    "        # Prepare the cy_df data\n",
    "        X_cy = cy_df[features]\n",
    "        y_cy = cy_df[config.target]\n",
    "        X_cy = pipeline.transform(X_cy)\n",
    "        y_cy = y_cy.values\n",
    "\n",
    "        evaluate_and_log_metrics(model, X_cy, y_cy, \"cy\")\n",
    "\n",
    "        # # Evaluate betting metrics\n",
    "        # evaluate_and_log_metrics_betting(cy_df, \"cy\", model, pipeline, features, config.bet_strat, config.margin)\n",
    "        # evaluate_and_log_metrics_betting(df_train, \"train\", model, pipeline, features, config.bet_strat, config.margin)\n",
    "        # evaluate_and_log_metrics_betting(df_val, \"val\", model, pipeline, features, config.bet_strat, config.margin)\n",
    "        # evaluate_and_log_metrics_betting(df_test, \"test\", model, pipeline, features, config.bet_strat, config.margin)\n",
    "\n",
    "\n",
    "# sweep_id = create_sweep()\n",
    "# print(sweep_id)\n",
    "# wandb.agent(sweep_id, train, count=50)\n",
    "wandb.agent(sweep_id='grantbell/nfl_bet_sweep_2/qm2m2u47', function=train, count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afIxw-p2TW6Z"
   },
   "outputs": [],
   "source": [
    "# @title get and test wandb models\n",
    "\"\"\"\n",
    "Module for evaluating NFL betting strategies using W&B run artifacts.\n",
    "\n",
    "This module contains helper functions to:\n",
    "  - Retrieve the top N runs from a W&B project based on a specified performance metric.\n",
    "  - Download and load model and preprocessing pipeline artifacts.\n",
    "  - Evaluate betting performance (profit, bet, ROI) on train, validation, test, and optional current-year data.\n",
    "  - Execute evaluations across multiple runs and betting strategies.\n",
    "  - Evaluate a single run's model performance on different dataset splits.\n",
    "\n",
    "High-Level Flow Diagram:\n",
    "----------------------------------------------------\n",
    "run_pipeline()\n",
    "   \u251c\u2500\u2500 get_top_runs_quickly()\n",
    "   \u2514\u2500\u2500 exe()\n",
    "         \u251c\u2500\u2500 load_model_and_pipeline()\n",
    "         \u251c\u2500\u2500 prepare_df()  <-- external function\n",
    "         \u2514\u2500\u2500 evaluate_betting_results()\n",
    "----------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Pipeline Entry Points\n",
    "# =============================================================================\n",
    "\n",
    "# def run_pipeline(project_path, features, top_metric='loss', top_n=10,\n",
    "#                  train_weight=1.0, wandb_project='nfl_bet_sweep_8',\n",
    "#                  bet_strats=None, margins=None, cy_df=None,\n",
    "#                  exclude_tested: bool = True):\n",
    "def run_pipeline(wandb_project, features, top_metric='loss', top_n=10,\n",
    "                 train_weight=1.0,\n",
    "                 bet_strats=None, margins=None, cy_df=None,\n",
    "                 exclude_tested: bool = True,\n",
    "                 pull_high_roi: bool = False,\n",
    "                 metric_threshold=0.60):\n",
    "    \"\"\"\n",
    "    Convenience function to run the entire evaluation pipeline:\n",
    "      1. Retrieve the top N runs from W&B based on the specified metric.\n",
    "      2. Execute betting evaluation across different strategies and margins.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results from all runs and evaluation settings.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve top runs from W&B\n",
    "    top_runs_df = get_top_runs_quickly(\n",
    "        # project_path=project_path,\n",
    "        wandb_project=wandb_project,\n",
    "        metric=top_metric,\n",
    "        top_n=top_n,\n",
    "        train_weight=train_weight,\n",
    "        metric_threshold=metric_threshold\n",
    "    )\n",
    "\n",
    "    # # Save the top runs information to CSV for record keeping\n",
    "    # top_runs_df.to_csv('top_runs.csv', index=False)\n",
    "\n",
    "    # Step 2: Execute the betting evaluation for the selected runs\n",
    "    results_df = exe(\n",
    "        df_runs_epochs=top_runs_df,\n",
    "        # base_df=base_df,\n",
    "        features=features,\n",
    "        wandb_project=wandb_project,\n",
    "        bet_strats=bet_strats,\n",
    "        margins=margins,\n",
    "        cy_df=cy_df\n",
    "        , exclude_tested=exclude_tested\n",
    "        , pull_high_roi=pull_high_roi\n",
    "    )\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_top_runs_quickly(\n",
    "    # project_path='nfl_bet_sweep_8',\n",
    "    wandb_project,\n",
    "    # Random guessing baseline: around 0.693 (because log(0.5)=0.693\u2212log(0.5)=0.693). If your positive class is only 33%, then a random guess loss would be closer to .636.\n",
    "    metric_threshold,\n",
    "    metric='loss',\n",
    "    # top_n=10,\n",
    "    top_n=None,\n",
    "    train_weight=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve and process only the top N runs for a project based on a specified metric,\n",
    "    but first filter out any runs whose aggregate metric doesn't meet the threshold.\n",
    "\n",
    "    Supported metrics: 'loss', 'accuracy', 'precision', 'recall', 'f1'.\n",
    "    For 'loss', lower values are better. For the other metrics, higher values are better.\n",
    "\n",
    "    Args:\n",
    "        project_path (str): W&B project path, e.g. \"user/project\".\n",
    "        metric (str): which metric to rank on.\n",
    "        top_n (int): how many runs to return.\n",
    "        train_weight (float): weight to give the training metric.\n",
    "        metric_threshold (float): per-split cutoff; actual cutoff on aggregate is\n",
    "                                  metric_threshold * (train_weight + 1 + 1).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame of the top runs (after thresholding), with full summary + config.\n",
    "    \"\"\"\n",
    "    # Validate metric choice\n",
    "    allowed_metrics = {'loss', 'accuracy', 'precision', 'recall', 'f1'}\n",
    "    if metric not in allowed_metrics:\n",
    "        raise ValueError(f\"Invalid metric '{metric}'. Choose from {allowed_metrics}.\")\n",
    "\n",
    "    lowest_is_better = (metric == 'loss')\n",
    "\n",
    "    api = wandb.Api()\n",
    "    # runs = api.runs(project_path)\n",
    "    runs = api.runs(wandb_project)\n",
    "\n",
    "    # gather per-run train/val/test\n",
    "    run_summaries = []\n",
    "    for run in runs:\n",
    "        s = run.summary._json_dict\n",
    "        run_summaries.append({\n",
    "            'run_id': run.id,\n",
    "            'run_name': run.name,\n",
    "            f'train_{metric}': s.get(f'train_{metric}'),\n",
    "            f'val_{metric}':   s.get(f'val_{metric}'),\n",
    "            f'test_{metric}':  s.get(f'test_{metric}')\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(run_summaries).dropna(\n",
    "        subset=[f'train_{metric}', f'val_{metric}', f'test_{metric}']\n",
    "    )\n",
    "\n",
    "    # compute aggregate\n",
    "    df['aggregate_metric'] = (\n",
    "        train_weight * df[f'train_{metric}'] +\n",
    "        df[f'val_{metric}'] +\n",
    "        df[f'test_{metric}']\n",
    "    )\n",
    "\n",
    "    df = df.sort_values(by='aggregate_metric', ascending=lowest_is_better)\n",
    "\n",
    "    # calculate total threshold\n",
    "    total_weight = train_weight + 2  # train + val + test\n",
    "    cutoff = metric_threshold * total_weight\n",
    "\n",
    "    # pre-filter by threshold\n",
    "    if lowest_is_better:\n",
    "        df = df[df['aggregate_metric'] <= cutoff]\n",
    "    else:\n",
    "        df = df[df['aggregate_metric'] >= cutoff]\n",
    "\n",
    "    print(f\"Filtered to {len(df)} runs.\")\n",
    "\n",
    "    if top_n:\n",
    "      # then pick top_n\n",
    "      if lowest_is_better:\n",
    "          df = df.nsmallest(top_n, 'aggregate_metric')\n",
    "      else:\n",
    "          df = df.nlargest(top_n, 'aggregate_metric')\n",
    "\n",
    "    # fetch full details for each\n",
    "    details = []\n",
    "    for _, row in df.iterrows():\n",
    "        # run = api.run(f\"{project_path}/{row['run_id']}\")\n",
    "        run = api.run(f\"{wandb_project}/{row['run_id']}\")\n",
    "        summary = run.summary._json_dict\n",
    "        config = {k: v for k, v in run.config.items() if not k.startswith('_')}\n",
    "        artifacts = [a.name for a in run.logged_artifacts() if '-history' not in a.name]\n",
    "        details.append({\n",
    "            **summary,\n",
    "            **config,\n",
    "            'run_id': run.id,\n",
    "            'run_name': run.name,\n",
    "            'last_artifact_name': artifacts[-1] if artifacts else None\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(details)\n",
    "\n",
    "def load_model_and_pipeline(run_id, last_artifact_name, wandb_project):\n",
    "    \"\"\"\n",
    "    Download and load a model and its corresponding preprocessing pipeline from W&B artifacts.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, pipeline) where model is a TensorFlow Keras model and pipeline is a pre-processing pipeline.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "\n",
    "    # Download and load the model artifact\n",
    "    model_artifact = api.artifact(f'grantbell/{wandb_project}/{last_artifact_name}')\n",
    "    artifact_dir_model = model_artifact.download()\n",
    "    model = tf.keras.models.load_model(f\"{artifact_dir_model}/model.keras\")\n",
    "\n",
    "    # Download and load the preprocessing pipeline artifact (assumes a .pkl file)\n",
    "    pipeline_artifact = api.artifact(f'grantbell/{wandb_project}/preprocessing_pipeline_{run_id}:v0')\n",
    "    artifact_dir_pipeline = pipeline_artifact.download()\n",
    "    pipeline_path = f\"{artifact_dir_pipeline}/pipeline.pkl\"\n",
    "    pipeline = load(pipeline_path)\n",
    "\n",
    "    # Optional: print artifact name for logging\n",
    "    print(last_artifact_name)\n",
    "\n",
    "    return model, pipeline\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Evaluation Functions\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_betting_results(model, pipeline, features, df_train, df_val, df_test, bet_strat, margin, cy_df=None):\n",
    "    \"\"\"\n",
    "    Evaluate the betting strategy using the provided model and pipeline on train, validation, test,\n",
    "    and optionally current-year data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Aggregated metrics including profit, total bet, and ROI for each dataset.\n",
    "    \"\"\"\n",
    "    # Evaluate betting performance on the training set\n",
    "    train_res = evaluate_betting_strategy(\n",
    "        df=df_train,\n",
    "        model=model,\n",
    "        features=features,\n",
    "        pipeline=pipeline,\n",
    "        bet_strat=bet_strat,\n",
    "        margin=margin\n",
    "    )\n",
    "    train_metrics = {\n",
    "        \"train_profit\": train_res[\"total_profit\"],\n",
    "        \"train_bet\":    train_res[\"total_bet\"],\n",
    "        \"train_roi\":    train_res[\"roi\"],\n",
    "        \"train_bet_rate\": train_res[\"bet_rate\"]\n",
    "    }\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    val_res = evaluate_betting_strategy(\n",
    "        df=df_val,\n",
    "        model=model,\n",
    "        features=features,\n",
    "        pipeline=pipeline,\n",
    "        bet_strat=bet_strat,\n",
    "        margin=margin\n",
    "    )\n",
    "    val_metrics = {\n",
    "        \"val_profit\": val_res[\"total_profit\"],\n",
    "        \"val_bet\":    val_res[\"total_bet\"],\n",
    "        \"val_roi\":    val_res[\"roi\"],\n",
    "        \"val_bet_rate\": val_res[\"bet_rate\"]\n",
    "    }\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_res = evaluate_betting_strategy(\n",
    "        df=df_test,\n",
    "        model=model,\n",
    "        features=features,\n",
    "        pipeline=pipeline,\n",
    "        bet_strat=bet_strat,\n",
    "        margin=margin\n",
    "    )\n",
    "    test_metrics = {\n",
    "        \"test_profit\": test_res[\"total_profit\"],\n",
    "        \"test_bet\":    test_res[\"total_bet\"],\n",
    "        \"test_roi\":    test_res[\"roi\"],\n",
    "        \"test_bet_rate\": test_res[\"bet_rate\"]\n",
    "    }\n",
    "\n",
    "    # If current-year data is provided, evaluate it as well\n",
    "    cy_metrics = {}\n",
    "    if cy_df is not None:\n",
    "        cy_res = evaluate_betting_strategy(\n",
    "            df=cy_df,\n",
    "            model=model,\n",
    "            features=features,\n",
    "            pipeline=pipeline,\n",
    "            bet_strat=bet_strat,\n",
    "            margin=margin\n",
    "        )\n",
    "        cy_metrics = {\n",
    "            \"cy_profit\": cy_res[\"total_profit\"],\n",
    "            \"cy_bet\":    cy_res[\"total_bet\"],\n",
    "            \"cy_roi\":    cy_res[\"roi\"],\n",
    "            \"cy_bet_rate\": cy_res[\"bet_rate\"]\n",
    "        }\n",
    "\n",
    "    # Combine and return all metrics\n",
    "    return {**train_metrics, **val_metrics, **test_metrics, **cy_metrics}\n",
    "\n",
    "import re # Import regular expressions for tag parsing\n",
    "\n",
    "def exe(df_runs_epochs, features, wandb_project='nfl_bet_sweep_8',\n",
    "        bet_strats=None, margins=None, cy_df=None,\n",
    "        exclude_tested: bool = True,\n",
    "        pull_high_roi: bool = False):\n",
    "    \"\"\"\n",
    "    Iterate over runs, load models/pipelines, prepare data, and evaluate betting strategies.\n",
    "\n",
    "    If pull_high_roi is True, only evaluates strategy/margin combinations found\n",
    "    in the run's 'high_roi_S_<strat>_M_<margin>' tags. Otherwise, evaluates all\n",
    "    combinations provided in bet_strats and margins.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results from all evaluated runs, strategies, and margins.\n",
    "    \"\"\"\n",
    "    # Set default betting strategies and margins if none provided\n",
    "    # These defaults are primarily used when pull_high_roi is False\n",
    "    if bet_strats is None:\n",
    "        bet_strats = ['both', 'dog', 'fav']\n",
    "    if margins is None:\n",
    "        margins = [0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "    results_list = []\n",
    "    api = wandb.Api() # Initialize API once\n",
    "\n",
    "    # Iterate over each run in the provided DataFrame\n",
    "    for _, row in df_runs_epochs.iterrows():\n",
    "        run_name = row['run_name']\n",
    "        run_id = row['run_id']\n",
    "        last_artifact_name = row['last_artifact_name']\n",
    "\n",
    "        print(f\"\\nProcessing run '{run_name}' (ID: {run_id})...\")\n",
    "\n",
    "        # --- Fetch Run Object (Optimized: Fetch only if needed) ---\n",
    "        run_obj = None\n",
    "        if exclude_tested or pull_high_roi:\n",
    "            try:\n",
    "                run_obj = api.run(f\"{wandb_project}/{run_id}\")\n",
    "            except wandb.errors.CommError as e:\n",
    "                print(f\"Error fetching run {run_id} from project {wandb_project}. Skipping. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # --- Filtering Logic ---\n",
    "        # 1. Skip if already tested (if exclude_tested is True)\n",
    "        if exclude_tested and run_obj and 'tested' in run_obj.tags:\n",
    "            print(f\"Skipping already tested run '{run_name}' (ID: {run_id}).\")\n",
    "            continue\n",
    "\n",
    "        # 2. Skip if artifact name contains \"history\"\n",
    "        if \"history\" in str(last_artifact_name).lower():\n",
    "            print(f\"Skipping run '{run_name}' with artifact '{last_artifact_name}' (contains 'history').\")\n",
    "            continue\n",
    "\n",
    "        # 3. Check for 'high_roi*' tags if pull_high_roi is True\n",
    "        high_roi_combos_to_evaluate = set() # Use a set to store unique (strat, margin) pairs\n",
    "        if pull_high_roi:\n",
    "            if not run_obj: # Should have been fetched above, but double check\n",
    "                 print(f\"Error: Could not fetch run object for {run_id} when pull_high_roi=True. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            has_any_high_roi_tag = False\n",
    "            # Regex to capture strategy and margin from tags like 'high_roi_S_fav_M_50'\n",
    "            tag_pattern = re.compile(r\"high_roi_S_(\\w+)_M_(\\d+)\")\n",
    "\n",
    "            for tag in run_obj.tags:\n",
    "                match = tag_pattern.match(tag)\n",
    "                if match:\n",
    "                    has_any_high_roi_tag = True\n",
    "                    strategy = match.group(1)\n",
    "                    margin_value = int(match.group(2))\n",
    "                    margin_float = margin_value / 1000.0 # Convert back to float (e.g., 50 -> 0.05)\n",
    "                    high_roi_combos_to_evaluate.add((strategy, margin_float))\n",
    "                    print(f\"  Found high ROI combo from tag '{tag}': Strategy='{strategy}', Margin={margin_float}\")\n",
    "\n",
    "            if not has_any_high_roi_tag:\n",
    "                print(f\"Skipping run '{run_name}' (ID: {run_id}) as pull_high_roi=True and no 'high_roi_S_*_M_*' tags were found.\")\n",
    "                continue\n",
    "            elif not high_roi_combos_to_evaluate:\n",
    "                 print(f\"Skipping run '{run_name}' (ID: {run_id}) as no valid combos could be parsed from tags.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "        # --- Load Model and Prepare Data (only if not skipped) ---\n",
    "        try:\n",
    "            model, pipeline = load_model_and_pipeline(run_id, last_artifact_name, wandb_project)\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading model/pipeline for run {run_id}. Skipping. Error: {e}\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        random_state = row['random_state']\n",
    "        min_periods = row['min_periods']\n",
    "        span = row['span']\n",
    "        print(f\"  Run Params: random_state={random_state}, min_periods={min_periods}, span={span}\")\n",
    "\n",
    "        # Prepare the dataset\n",
    "        df_prepared = prepare_df(min_periods=min_periods, span=span)\n",
    "        cy = 2024 # Consider making this dynamic or a parameter\n",
    "        # current_cy_df = df_prepared[df_prepared['season'] == cy].copy() if cy_df is None else cy_df.copy()\n",
    "        current_cy_df = df_prepared[df_prepared['season'] == cy].copy()\n",
    "        df_prepared = df_prepared[df_prepared['season'] < cy]\n",
    "\n",
    "        # Split the prepared dataset\n",
    "        target = 'dog_win'\n",
    "        test_size = 0.1\n",
    "        if df_prepared.empty or len(df_prepared[target].unique()) < 2:\n",
    "             print(f\"Skipping run {run_id} due to insufficient data or classes for splitting after filtering.\")\n",
    "             continue\n",
    "\n",
    "        X = df_prepared[features]\n",
    "        y = df_prepared[target]\n",
    "\n",
    "        # Ensure enough samples for stratification\n",
    "        if y.value_counts().min() < 2:\n",
    "             print(f\"Skipping run {run_id}: Not enough samples in the smallest class for stratification.\")\n",
    "             continue\n",
    "\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Ensure enough samples for the second split\n",
    "        if y_train_df.value_counts().min() < 2:\n",
    "             print(f\"Skipping run {run_id}: Not enough samples in the smallest class for validation split stratification.\")\n",
    "             continue\n",
    "\n",
    "        X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(\n",
    "            X_train_df, y_train_df, test_size=test_size, stratify=y_train_df, random_state=random_state\n",
    "        )\n",
    "\n",
    "        df_train = df_prepared.loc[X_train_df.index].copy()\n",
    "        df_val = df_prepared.loc[X_val_df.index].copy()\n",
    "        df_test = df_prepared.loc[X_test_df.index].copy()\n",
    "\n",
    "\n",
    "        # --- Evaluation Loop ---\n",
    "        run_evaluated = False\n",
    "        if pull_high_roi:\n",
    "            # Evaluate ONLY the combinations extracted from tags\n",
    "            print(f\"  Evaluating {len(high_roi_combos_to_evaluate)} high ROI combination(s) found in tags...\")\n",
    "            for bet_strat, margin in high_roi_combos_to_evaluate:\n",
    "                 print(f\"    Evaluating: Strategy='{bet_strat}', Margin={margin}\")\n",
    "                 try:\n",
    "                     eval_dict = evaluate_betting_results(\n",
    "                         model=model, pipeline=pipeline, features=features,\n",
    "                         df_train=df_train, df_val=df_val, df_test=df_test,\n",
    "                         bet_strat=bet_strat, margin=margin, cy_df=current_cy_df\n",
    "                     )\n",
    "                     eval_dict.update({\n",
    "                         'bet_strat': bet_strat, 'margin': margin, 'run_id': run_id,\n",
    "                         'last_artifact_name': last_artifact_name, 'run_name': run_name\n",
    "                     })\n",
    "                     results_list.append(eval_dict)\n",
    "                     run_evaluated = True # Mark that at least one evaluation was done for this run\n",
    "                 except Exception as e:\n",
    "                     print(f\"    Error during evaluation for combo (Strat: {bet_strat}, Margin: {margin}): {e}\")\n",
    "        else:\n",
    "            # Evaluate ALL combinations from the provided lists (original behavior)\n",
    "            print(f\"  Evaluating all {len(bet_strats)} strategies and {len(margins)} margins...\")\n",
    "            for bet_strat in bet_strats:\n",
    "                for margin in margins:\n",
    "                    print(f\"    Evaluating: Strategy='{bet_strat}', Margin={margin}\")\n",
    "                    try:\n",
    "                        eval_dict = evaluate_betting_results(\n",
    "                            model=model, pipeline=pipeline, features=features,\n",
    "                            df_train=df_train, df_val=df_val, df_test=df_test,\n",
    "                            bet_strat=bet_strat, margin=margin, cy_df=current_cy_df\n",
    "                        )\n",
    "                        eval_dict.update({\n",
    "                            'bet_strat': bet_strat, 'margin': margin, 'run_id': run_id,\n",
    "                            'last_artifact_name': last_artifact_name, 'run_name': run_name\n",
    "                        })\n",
    "\n",
    "                        # --- Tagging Logic (Only when NOT pulling high ROI) ---\n",
    "                        train_roi = eval_dict.get('train_roi', -float('inf'))\n",
    "                        test_roi = eval_dict.get('test_roi', -float('inf'))\n",
    "                        val_roi = eval_dict.get('val_roi', -float('inf'))\n",
    "\n",
    "                        if train_roi > 7.5 and test_roi > 7.5 and val_roi > 7.5:\n",
    "                            if not run_obj: # Fetch run_obj if not already done\n",
    "                                try:\n",
    "                                     run_obj = api.run(f\"{wandb_project}/{run_id}\")\n",
    "                                except wandb.errors.CommError as e:\n",
    "                                     print(f\"    Error fetching run {run_id} for tagging. Tagging skipped. Error: {e}\")\n",
    "                                     continue # Skip tagging for this combo\n",
    "\n",
    "                            if run_obj: # Check if fetch was successful\n",
    "                                try:\n",
    "                                    margin_tag_value = int(margin * 1000)\n",
    "                                    tag_name = f\"high_roi_S_{bet_strat}_M_{margin_tag_value}\"\n",
    "\n",
    "                                    if tag_name not in run_obj.tags:\n",
    "                                        run_obj.tags.append(tag_name)\n",
    "                                        run_obj.update() # Persist tag change\n",
    "                                        print(f\"    Tagged run '{run_name}' with '{tag_name}'.\")\n",
    "                                    # else: # Optional: print if tag already exists\n",
    "                                    #     print(f\"    Tag '{tag_name}' already exists on run '{run_name}'.\")\n",
    "                                except Exception as e:\n",
    "                                    print(f\"    Error adding tag '{tag_name}' to run '{run_name}': {e}\")\n",
    "\n",
    "                        results_list.append(eval_dict)\n",
    "                        run_evaluated = True # Mark that at least one evaluation was done\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error during evaluation for combo (Strat: {bet_strat}, Margin: {margin}): {e}\")\n",
    "\n",
    "\n",
    "        # --- Add 'tested' Tag (if any evaluation was performed for this run) ---\n",
    "        if run_evaluated and run_obj and 'tested' not in run_obj.tags:\n",
    "             try:\n",
    "                 print(f\"  Marking run '{run_name}' as 'tested'.\")\n",
    "                 run_obj.tags.append('tested')\n",
    "                 run_obj.update()\n",
    "             except Exception as e:\n",
    "                 print(f\"  Error adding 'tested' tag to run '{run_name}': {e}\")\n",
    "        elif run_evaluated and not run_obj:\n",
    "             print(f\"  Warning: Run {run_id} was evaluated but could not be marked as 'tested' (run object not available).\")\n",
    "\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    if not results_list:\n",
    "        print(\"\\nNo results were generated.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame if no results\n",
    "    else:\n",
    "        print(f\"\\nFinished processing. Combining {len(results_list)} results.\")\n",
    "        return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "def evaluate_single_run(\n",
    "    # project_path: str,\n",
    "    run_id: str,\n",
    "    features: list[str],\n",
    "    bet_strat: str,\n",
    "    margin: float,\n",
    "    # wandb_project: str = 'nfl_bet_sweep_8',\n",
    "    wandb_project: str,\n",
    "    cy_year: int = 2024,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate one W&B run with one betting strategy + margin.\n",
    "    Raises KeyError if 'random_state', 'min_periods', or 'span' are missing\n",
    "    (no defaults allowed).\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    # run = api.run(f\"{project_path}/{run_id}\")\n",
    "    run = api.run(f\"grantbell/{wandb_project}/{run_id}\")\n",
    "\n",
    "    # 1) grab the flat summary dict\n",
    "    summary = run.summary._json_dict\n",
    "\n",
    "    # 2) grab config, filtering out private keys\n",
    "    config = {k: v for k, v in run.config.items() if not k.startswith('_')}\n",
    "\n",
    "    # 3) merge exactly as get_top_runs_quickly does\n",
    "    details = {**summary, **config}\n",
    "\n",
    "    # 4) enforce no-default requirement\n",
    "    required = ['random_state', 'min_periods', 'span']\n",
    "    missing = [k for k in required if k not in details]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"Run '{run_id}' missing required key(s): {missing}. \"\n",
    "            f\"Available in summary+config: {list(details.keys())}\"\n",
    "        )\n",
    "\n",
    "    random_state = details['random_state']\n",
    "    min_periods  = details['min_periods']\n",
    "    span         = details['span']\n",
    "\n",
    "    # 5) pick last non-history artifact\n",
    "    artifact_names = [\n",
    "        art.name for art in run.logged_artifacts()\n",
    "        if '-history' not in art.name.lower()\n",
    "    ]\n",
    "    if not artifact_names:\n",
    "        raise ValueError(f\"No non-history artifact found for run {run_id}\")\n",
    "    last_artifact_name = artifact_names[-1]\n",
    "\n",
    "    # 6) load model & pipeline\n",
    "    model, pipeline = load_model_and_pipeline(run_id, last_artifact_name, wandb_project)\n",
    "\n",
    "    # Prepare the dataset using an external function (assumed to exist)\n",
    "    df_prepared = prepare_df(min_periods=min_periods, span=span)\n",
    "    cy = 2024\n",
    "    cy_df = df_prepared[df_prepared['season'] == cy]\n",
    "    df_prepared = df_prepared[df_prepared['season'] < cy]\n",
    "\n",
    "    # Split the prepared dataset into train, test, and validation sets using stratification\n",
    "    target = 'dog_win'\n",
    "    test_size = 0.1\n",
    "\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "        df_prepared[features],\n",
    "        df_prepared[target],\n",
    "        test_size=test_size,\n",
    "        stratify=df_prepared[target],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(\n",
    "        X_train_df,\n",
    "        y_train_df,\n",
    "        test_size=test_size,\n",
    "        stratify=y_train_df,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Subset the corresponding rows from the prepared DataFrame\n",
    "    df_train = df_prepared.loc[X_train_df.index].copy()\n",
    "    df_val   = df_prepared.loc[X_val_df.index].copy()\n",
    "    df_test  = df_prepared.loc[X_test_df.index].copy()\n",
    "\n",
    "    test_res = evaluate_betting_strategy(\n",
    "        df=df_test,\n",
    "        model=model,\n",
    "        features=features,\n",
    "        pipeline=pipeline,\n",
    "        bet_strat=bet_strat,\n",
    "        margin=margin\n",
    "    )\n",
    "\n",
    "    cy_res = evaluate_betting_strategy(\n",
    "        df=cy_df,\n",
    "        model=model,\n",
    "        features=features,\n",
    "        pipeline=pipeline,\n",
    "        bet_strat=bet_strat,\n",
    "        margin=margin\n",
    "    )\n",
    "\n",
    "    return test_res, cy_res\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "def filter_results_df(\n",
    "    df: pd.DataFrame,\n",
    "    train_roi_min: float = 0,\n",
    "    val_test_roi_min: float = 0,\n",
    "    cy_roi_min: Optional[float] = None,\n",
    "    train_bet_min: Optional[float] = None,\n",
    "    val_bet_min: Optional[float] = None,\n",
    "    test_bet_min: Optional[float] = None,\n",
    "    cy_bet_min: Optional[float] = None,\n",
    "    test_bet_rate_min: Optional[float] = None,\n",
    "    cols: List[str] = [\n",
    "        'run_name', 'run_id',\n",
    "        'train_profit', 'val_profit', 'test_profit', 'cy_profit',\n",
    "        'train_roi',    'val_roi',    'test_roi',    'cy_roi',\n",
    "        'train_bet_rate', 'val_bet_rate', 'test_bet_rate', 'cy_bet_rate',\n",
    "        'cy_bet', 'bet_strat', 'margin'\n",
    "    ]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter by ROI and bet thresholds, then select only the columns in `cols`.\n",
    "\n",
    "    Args:\n",
    "      df:               your runs DataFrame\n",
    "      train_roi_min:    minimum train_roi\n",
    "      val_test_roi_min: minimum val_roi and test_roi\n",
    "      cy_roi_min:       optional minimum current-year ROI\n",
    "      train_bet_min:    optional minimum train_bet\n",
    "      val_bet_min:      optional minimum val_bet\n",
    "      test_bet_min:     optional minimum test_bet\n",
    "      cy_bet_min:       optional minimum cy_bet\n",
    "      cols:             list of columns to return (default as above)\n",
    "\n",
    "    Returns:\n",
    "      A filtered DataFrame with only the requested columns.\n",
    "    \"\"\"\n",
    "    # build row-filter mask\n",
    "    mask = (\n",
    "        (df['train_roi'] >= train_roi_min) &\n",
    "        (df['val_roi']   >= val_test_roi_min) &\n",
    "        (df['test_roi']  >= val_test_roi_min)\n",
    "    )\n",
    "\n",
    "    if cy_roi_min is not None:\n",
    "        mask &= (df['cy_roi'] >= cy_roi_min)\n",
    "    if train_bet_min is not None:\n",
    "        mask &= (df['train_bet'] >= train_bet_min)\n",
    "    if val_bet_min is not None:\n",
    "        mask &= (df['val_bet'] >= val_bet_min)\n",
    "    if test_bet_min is not None:\n",
    "        mask &= (df['test_bet'] >= test_bet_min)\n",
    "    if cy_bet_min is not None:\n",
    "        mask &= (df['cy_bet'] >= cy_bet_min)\n",
    "    if test_bet_rate_min is not None:\n",
    "        mask &= (df['test_bet_rate'] >= test_bet_rate_min)\n",
    "\n",
    "    # subset rows and then columns\n",
    "    return df.loc[mask, cols].copy()\n",
    "\n",
    "from typing import Dict, Optional\n",
    "\n",
    "def select_best_by_avg_roi(\n",
    "    df: pd.DataFrame,\n",
    "    min_train_bet_rate: Optional[Dict[str, float]] = None,\n",
    "    min_test_bet_rate: Optional[Dict[str, float]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 0. apply bet-rate filters if provided ---\n",
    "    if min_train_bet_rate is not None:\n",
    "        # map each row's strategy to its threshold, default to -inf so\n",
    "        # unspecified strategies pass\n",
    "        train_thresh = df['bet_strat'].map(min_train_bet_rate).fillna(-np.inf)\n",
    "        df = df.loc[df['train_bet_rate'] >= train_thresh]\n",
    "\n",
    "    if min_test_bet_rate is not None:\n",
    "        test_thresh = df['bet_strat'].map(min_test_bet_rate).fillna(-np.inf)\n",
    "        df = df.loc[df['test_bet_rate']  >= test_thresh]\n",
    "\n",
    "    # --- 1. Compute per\u2010row average ROI ---\n",
    "    roi_cols = ['train_roi', 'val_roi', 'test_roi']\n",
    "    df['avg_roi'] = df[roi_cols].mean(axis=1)\n",
    "\n",
    "    # --- 2. Pick best row per run_id \u00d7 bet_strat ---\n",
    "    best_idx = df.groupby(['run_id', 'bet_strat'])['avg_roi'].idxmax()\n",
    "\n",
    "    df.sort_values(by='avg_roi', ascending=False, inplace=True)\n",
    "\n",
    "    # --- 3. Slice, clean up, and return ---\n",
    "    result = (\n",
    "        df\n",
    "        .loc[best_idx]\n",
    "        .drop(columns='avg_roi')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMlcJwj1xCnX"
   },
   "outputs": [],
   "source": [
    "results_df = run_pipeline(\n",
    "    wandb_project='nfl_bet_sweep_2',\n",
    "    features=features,\n",
    "    top_metric='loss',\n",
    "    top_n=None,\n",
    "    bet_strats=['dog','fav'],\n",
    "    margins=[0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1],\n",
    "    cy_df=cy_df\n",
    "    , metric_threshold=.60\n",
    "    , exclude_tested = True\n",
    "    # , exclude_tested = False\n",
    "    # , pull_high_roi = True\n",
    ")\n",
    "\n",
    "filtered_df = filter_results_df(results_df, train_roi_min=7.5, val_test_roi_min=7.5)\n",
    "\n",
    "# Show unique run_id values nicely\n",
    "unique_run_ids = filtered_df['run_name'].unique()\n",
    "\n",
    "# Print them out line by line\n",
    "for run_id in unique_run_ids:\n",
    "    print(run_id)\n",
    "\n",
    "# filtered_df = select_best_by_avg_roi(filtered_df)\n",
    "filtered_df = select_best_by_avg_roi(\n",
    "    filtered_df,\n",
    "    min_test_bet_rate ={'dog': 100/60, 'fav': 100/30},\n",
    ")\n",
    "\n",
    "# every two weeks is probably too infrequent for fave but also there might be noise in teh specific test group selected so don't want to lose a quality model. Might need to increase.\n",
    "filtered_df[(filtered_df['bet_strat']=='fav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpluZLu-KmZv"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poNsp-6VJ7uO"
   },
   "outputs": [],
   "source": [
    "results_df = run_pipeline(\n",
    "    wandb_project='nfl_bet_sweep_2',\n",
    "    features=features,\n",
    "    top_metric='loss',\n",
    "    top_n=None,\n",
    "    bet_strats=['dog','fav'],\n",
    "    margins=[0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1],\n",
    "    cy_df=cy_df\n",
    "    , metric_threshold=.60\n",
    "    # , exclude_tested = True\n",
    "    , exclude_tested = False\n",
    "    , pull_high_roi = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGSThDvRvoxQ"
   },
   "outputs": [],
   "source": [
    "filtered_df = filter_results_df(results_df, train_roi_min=7.5, val_test_roi_min=7.5)\n",
    "\n",
    "# filtered_df = select_best_by_avg_roi(filtered_df)\n",
    "filtered_df = select_best_by_avg_roi(\n",
    "    filtered_df,\n",
    "    min_test_bet_rate ={'dog': 100/60, 'fav': 100/30},\n",
    ")\n",
    "\n",
    "# every two weeks is probably too infrequent for fave but also there might be noise in teh specific test group selected so don't want to lose a quality model. Might need to increase.\n",
    "filtered_df[(filtered_df['bet_strat']=='fav')]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_df[(filtered_df['bet_strat']=='dog')]"
   ],
   "metadata": {
    "id": "ZoITgyq6n3Vi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gs_LoQ-1GMpC"
   },
   "outputs": [],
   "source": [
    "filtered_df = filter_results_df(results_df, train_roi_min=7.5, val_test_roi_min=7.5)\n",
    "\n",
    "# filtered_df = select_best_by_avg_roi(filtered_df)\n",
    "filtered_df = select_best_by_avg_roi(\n",
    "    filtered_df,\n",
    "    min_test_bet_rate ={'dog': 100/30, 'fav': 100/15},\n",
    ")\n",
    "\n",
    "filtered_df[(filtered_df['bet_strat']=='fav')]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_df[(filtered_df['bet_strat']=='dog')]"
   ],
   "metadata": {
    "id": "ZXu7eV-3oP3u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_df = filter_results_df(results_df, train_roi_min=10, val_test_roi_min=10)\n",
    "\n",
    "# filtered_df = select_best_by_avg_roi(filtered_df)\n",
    "filtered_df = select_best_by_avg_roi(\n",
    "    filtered_df,\n",
    "    min_test_bet_rate ={'dog': 100/60, 'fav': 100/30},\n",
    ")\n",
    "\n",
    "filtered_df[(filtered_df['bet_strat']=='fav')]"
   ],
   "metadata": {
    "id": "k_02bmSnpGC-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_df[(filtered_df['bet_strat']=='dog')]"
   ],
   "metadata": {
    "id": "C6Ciz_G7oXcK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWByvmdruT4p"
   },
   "outputs": [],
   "source": [
    "filtered_df = filter_results_df(results_df, train_roi_min=10, val_test_roi_min=10)\n",
    "\n",
    "# filtered_df = select_best_by_avg_roi(filtered_df)\n",
    "filtered_df = select_best_by_avg_roi(\n",
    "    filtered_df,\n",
    "    min_test_bet_rate ={'dog': 100/60, 'fav': 100/15},\n",
    ")\n",
    "\n",
    "filtered_df[(filtered_df['bet_strat']=='fav')]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_df[(filtered_df['bet_strat']=='dog')]"
   ],
   "metadata": {
    "id": "zg25wXFEonZC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-Lvk1BfkHtn"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRIRJ_XDLkD8"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIG ---\n",
    "bet_strat = 'fav'         # or 'dog'\n",
    "base_bet = 100            # dollars per bet\n",
    "# runs = {\n",
    "#     # dauntless-sweep-42\n",
    "#    '6po6v9zy': 0.055,\n",
    "#     # jumping-sweep-111\n",
    "#    'askoip2w': 0.065,\n",
    "#     # genial-sweep-346\n",
    "#    'biqlz8ar': 0.07,\n",
    "#     # leafy-sweep-389\n",
    "#    'j6q6o050': 0.065,\n",
    "#     # magic-sweep-125\n",
    "#    'm3jh0vdm': 0.065,\n",
    "# }\n",
    "runs = {\n",
    "    # dauntless-sweep-42\n",
    "   '6po6v9zy': 0.04,\n",
    "    # jumping-sweep-111\n",
    "   'askoip2w': 0.065,\n",
    "    # genial-sweep-346\n",
    "   'biqlz8ar': 0.06,\n",
    "    # atomic-sweep-547\n",
    "    'fs3i8481': 0.015,\n",
    "    # leafy-sweep-389\n",
    "   'j6q6o050': 0.065,\n",
    "    # likely-sweep-678\n",
    "    'pno9rh0z': 0.025\n",
    "}\n",
    "\n",
    "# keys we'll merge on\n",
    "KEYS = ['season','week','fav_team','dog_team','dog_win']\n",
    "\n",
    "# 1) First pass: grab the ground-truth profit column from any one run\n",
    "first_run, first_margin = next(iter(runs.items()))\n",
    "_, first_res = evaluate_single_run(\n",
    "    wandb_project='nfl_bet_sweep_2',\n",
    "    run_id=first_run,\n",
    "    features=features,\n",
    "    bet_strat=bet_strat,\n",
    "    margin=first_margin\n",
    ")\n",
    "profit_col = f'{bet_strat}_profit'\n",
    "profit_df = first_res['df'][KEYS + [profit_col]].rename(columns={profit_col: 'profit'})\n",
    "\n",
    "# 2) Build one dataframe per run containing only KEYS + flag_{run_id}\n",
    "flag_dfs = []\n",
    "for run_id, margin in runs.items():\n",
    "    _, res = evaluate_single_run(\n",
    "        wandb_project='nfl_bet_sweep_2',\n",
    "        run_id=run_id,\n",
    "        features=features,\n",
    "        bet_strat=bet_strat,\n",
    "        margin=margin\n",
    "    )\n",
    "    df = res['df'][KEYS + ['bet']].copy()\n",
    "    flag_col = f'flag_{run_id}'\n",
    "    df[flag_col] = df['bet'] > 0\n",
    "    flag_dfs.append(df[KEYS + [flag_col]])\n",
    "\n",
    "# 3) Merge all flag dataframes on the identifying KEYS\n",
    "merged_flags = reduce(\n",
    "    lambda L, R: pd.merge(L, R, on=KEYS, how='outer'),\n",
    "    flag_dfs\n",
    ").fillna(False)   # missing flags \u2192 False\n",
    "\n",
    "# 4) Attach the ground-truth profit column\n",
    "merged = pd.merge(merged_flags, profit_df, on=KEYS, how='left').fillna({'profit': 0})\n",
    "\n",
    "# 5) Compute per-run profit, bets, ROI\n",
    "profit_per_run = {}\n",
    "bets_per_run   = {}\n",
    "roi_per_run    = {}\n",
    "\n",
    "for run_id in runs:\n",
    "    fcol = f'flag_{run_id}'\n",
    "    p = merged.loc[merged[fcol], 'profit'].sum()\n",
    "    b = merged[fcol].sum()\n",
    "    profit_per_run[run_id] = p\n",
    "    bets_per_run[run_id]   = int(b)\n",
    "    roi_per_run[run_id]    = (p / (b * base_bet)) if b else float('nan')\n",
    "\n",
    "print(\"Per-run results:\")\n",
    "for run_id in runs:\n",
    "    print(f\" \u2022 {run_id}: profit={profit_per_run[run_id]:.2f}, \"\n",
    "          f\"bets={bets_per_run[run_id]}, \"\n",
    "          f\"ROI={roi_per_run[run_id]:.4f}\")\n",
    "\n",
    "# 6) Compute ensemble metrics for \"at least k\" votes\n",
    "merged['vote_count'] = merged[[f'flag_{r}' for r in runs]].sum(axis=1).astype(int)\n",
    "\n",
    "ensemble = {}\n",
    "N = len(runs)\n",
    "for k in range(1, N+1):\n",
    "    sub = merged[merged['vote_count'] >= k]   # <= change here\n",
    "    profit_k = sub['profit'].sum()\n",
    "    bets_k   = sub.shape[0]\n",
    "    roi_k    = (profit_k / (bets_k * base_bet)) if bets_k else float('nan')\n",
    "    ensemble[k] = {\n",
    "        'profit': float(profit_k),\n",
    "        'bets':   int(bets_k),\n",
    "        'ROI':    float(roi_k)\n",
    "    }\n",
    "\n",
    "print(\"\\nEnsemble (\u2265 k votes):\")\n",
    "for k, stats in ensemble.items():\n",
    "    print(f\" \u2022 k\u2265{k}: profit={stats['profit']:.2f}, \"\n",
    "          f\"bets={stats['bets']}, ROI={stats['ROI']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sozVy2ZYr3lv"
   },
   "outputs": [],
   "source": [
    "merged[merged['vote_count']>=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0ZXHlmIfxmk"
   },
   "outputs": [],
   "source": [
    "merged[merged['vote_count']>=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxSz-vQmKM3w"
   },
   "source": [
    "project 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw3LyEhO_HvY"
   },
   "outputs": [],
   "source": [
    "results_df = run_pipeline(\n",
    "    # wandb_project='nfl_bet_sweep_2',\n",
    "    wandb_project='nfl_bet_sweep_8',\n",
    "    features=features,\n",
    "    top_metric='loss',\n",
    "    top_n=None,\n",
    "    bet_strats=['dog','fav'],\n",
    "    margins=[0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1],\n",
    "    cy_df=cy_df\n",
    "    , exclude_tested = False\n",
    "    , pull_high_roi = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSGy8CeZCQID"
   },
   "outputs": [],
   "source": [
    "filtered_df = filter_results_df(results_df, train_roi_min=7.5, val_test_roi_min=7.5)\n",
    "\n",
    "# Show unique run_id values nicely\n",
    "unique_run_ids = filtered_df['run_name'].unique()\n",
    "\n",
    "# Print them out line by line\n",
    "for run_id in unique_run_ids:\n",
    "    print(run_id)\n",
    "\n",
    "print(len(filtered_df))\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "povnbS7OWjDK"
   },
   "outputs": [],
   "source": [
    "# filtered_df = select_best_by_avg_roi(filtered_df)\n",
    "filtered_df = select_best_by_avg_roi(\n",
    "    filtered_df,\n",
    "    # min_train_bet_rate={'dog': 0.02, 'fav': 0.05},\n",
    "    min_test_bet_rate ={'dog': 100/60, 'fav': 100/30},\n",
    ")\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leg6g8sL-8MF"
   },
   "outputs": [],
   "source": [
    "# every two weeks is probably too infrequent for fave but also there might be noise in teh specific test group selected so don't want to lose a quality model. Might need to increase.\n",
    "filtered_df[(filtered_df['bet_strat']=='fav') & (filtered_df['test_bet_rate']>100/30)]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}